{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "978fcaa1-643f-42b7-a63c-0d5ed380de3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f1d35bc-9941-44e8-b880-cad253d23ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We first define the Input embeddings as mentioned in the paper\n",
    "\n",
    "class InputEmbeddings(nn.Module):                     \n",
    "    # define the constructor\n",
    "    def __init__(self, d_model : int, vocab_size : int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self,x):                                       \n",
    "        return self.embedding(x) * math.sqrt(self.d_model)     # In the embedding layers the weights are multiplied by sqrt(d_model)\n",
    "\n",
    "class PositionalEmbeddings(nn.Module):\n",
    "    # define the constructor\n",
    "    def __init__(self, d_model : int, seq_len : int, dropout : float):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.se_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Create a mtrix of shape (seq_len, d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        # create a vector of shape (seq_len)\n",
    "        position = torch.arange (0,seq_len,dtype = torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model,2).float()* (-math.log(10000.0) / d_model))\n",
    "        # apply the sin to even positions \n",
    "        pe[:,0::2] = torch.sin(position * div_term)\n",
    "        pe[:,1::2] = torch.cos(position * div_term)\n",
    "        # We need to add the \" batch dimension \" to these tensors so that we can apply it to the whole sentences.\n",
    "        # there will be bacthes of sentences.\n",
    "\n",
    "        pe = pe.unsqueeze(0)   # it will become a tensor of dimension (1, Seq_Len, d_model)\n",
    "\n",
    "        # we can register this tensor in the buffer of this module\n",
    "        # Buffer of the module : when we want to keep a tensor inside the module but not as a learned parameter but \n",
    "        # when we want to save it when we save the file of the model we should register it as a buffer. \n",
    "        # This way the tensor will be saved in the file along with the state of the model.\n",
    "        \n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # we need to atatch the positiional encoding with every word and we also need to keep in mind that they dont get trained.\n",
    "        x = x + (self.pe[:, :x.shape[1], : ]).required_grad_(False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "#########################################  BUILDING THE ENCODER ######################\n",
    "# Next we are going to build the small small components of the encoder which are the multi-head attention, the normalization layer,\n",
    "# The feed forward network and the skip connection. \n",
    "\n",
    "# WE BEGIN WITH THE LAYER NORMALIZATION \n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self,eps : float = 10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(1))  # Multiplied\n",
    "        self.bias  = nn.Parameter(torch.zeros(1)) # Added\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim =1, keepdim = True)\n",
    "        std = x.std(dim =-1, keepdim = True)\n",
    "        return self.alpha* (x- mean) / (std + self.eps) + self.bias\n",
    "\n",
    "# THE FEED FORWARD NETWORK\n",
    "\n",
    "class FeedforwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model : int, d_ff: int, dropout : float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # W1 and B1\n",
    "        self.dropout  = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # W2 and B2\n",
    "\n",
    "    def forward(self,x):\n",
    "        # (Batch, Seq_len,d_model) --> (Batch, Seq_len, d_ff ) --> (Batch, Seq_len, d_model) \n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
    "\n",
    "\n",
    "# Multi-head attention block\n",
    "\n",
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        assert d_model %h == 0, \"d_model is not divisible by h\"\n",
    "        self.d_k = d_model // h\n",
    "        self.W_q = nn.Linear(d_model,d_model) # Wq\n",
    "        self.W_k = nn.Linear(d_model,d_model) # Wk\n",
    "        self.W_v = nn.Linear(d_model,d_model) # Wv\n",
    "\n",
    "        self.w_o = nn.Linear(d_model, d_model) #Wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key,value,mask, dropout: nn. Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "\n",
    "        # (Batch, h, Seq_len, d_k) --> (Batch, h, Seq_len, Seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2,-1)) /math.sqrt(d_k)\n",
    "        # before applying the softmax we need to apply the mask to hide. So we replace all those interactions with \n",
    "        # very very small values.\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill_(mask==0,-1e9)\n",
    "            \n",
    "        attention_scores = attention_scores.softmax(dim = -1) # (Batch, h, Seq_len, Seq_len)\n",
    "\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "\n",
    "        return (attention_scores @ value), attention_scores\n",
    "        \n",
    "\n",
    "    def forward(self,q,k,v,mask):\n",
    "         query = self.w_q(q)    # (Batch, Seq_len,d_model) --> (Batch, Seq_len, d_model)\n",
    "         key = self.w_k(k)      # (Batch, Seq_len,d_model) --> (Batch, Seq_len, d_model)\n",
    "         value = self.w_v(v)    # (Batch, Seq_len,d_model) --> (Batch, Seq_len, d_model)\n",
    "\n",
    "         # We want to split the embeddings not the sentence. \n",
    "         # (Batch,Seq_len,d_model) --> (Batch, Seq_len, h, d_k) -->(Batch, h, Seq_len, d_k)\n",
    "         query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "         key = key.view(key.shape[0], key.shape[1],self.h, self.d_k).transpose(1,2)\n",
    "         value = value.view(value.shape[0], value.shape[1],self.h,self.d_k).transpose(1,2)\n",
    "\n",
    "         x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "\n",
    "         # (batch, h, Seq_len, d_k) --> (Batch, Seq_len, h, d_K) --> (Batch, Seq_len, d_model)\n",
    "         x = x.transpose(1,2).contiguous().view(x.shape[0], -1,self.h*self.d_k)\n",
    "\n",
    "         # ()\n",
    "         return self.w_o(x)\n",
    "         \n",
    "\n",
    "         \n",
    "         \n",
    "\n",
    "         \n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ee3ab3-9f85-4b40-933c-cc17b77e7082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e9dd6f-24ca-44e9-84a8-53a19bf3a9f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec14522-8bf4-46cd-9a91-e5faf42ccee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70dde9e-9173-4972-9d07-97bcc0e4d419",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
